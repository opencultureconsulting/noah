# https://taskfile.dev

version: '3'

tasks:
  default:
    desc: Elpub Wuppertal harvesten und transformieren
    deps: [harvest]
    cmds:
      - task: refine
      - task: check
      - task: split
      - task: validate
      - task: zip
      - task: diff

  harvest:
    dir: data/wuppertal/harvest
    cmds:
      - METHA_DIR=$PWD metha-sync --format oai_dc http://elpub.bib.uni-wuppertal.de/servlets/OAIDataProvider
      - METHA_DIR=$PWD metha-cat --format oai_dc http://elpub.bib.uni-wuppertal.de/servlets/OAIDataProvider > wuppertal.xml

  refine:
    dir: data/wuppertal/refine
    ignore_error: true # provisorisch verwaisten Java-Prozess bei Exit vermeiden https://github.com/go-task/task/issues/141
    env:
      PORT: 3335
      RAM: 4G
      PROJECT: wuppertal
    cmds:
      # OpenRefine starten
      - $OPENREFINE -v warn -p $PORT -m $RAM -d $PWD > openrefine.log 2>&1 &
      - timeout 30s bash -c "until curl -s http://localhost:$PORT | cat | grep -q -o OpenRefine ; do sleep 1; done"
      # Import (erfordert absoluten Pfad zur XML-Datei)
      - $OPENREFINE_CLIENT -P $PORT --create "$(readlink -e ../harvest/wuppertal.xml)" --recordPath Records --recordPath Record --storeEmptyStrings false --trimStrings true --projectName $PROJECT
      # Vorverarbeitung: Identifier in erste Spalte; nicht benötigte Spalten (ohne differenzierende Merkmale) löschen; verbleibende Spalten umbenennen (Pfad entfernen)
      - $OPENREFINE_CLIENT -P $PORT --apply ../../../rules/wuppertal/vorverarbeitung.json $PROJECT
      # Entfernen von HTML-Tags und Transformation von subscript und superscript in Unicode (betrifft dc:description, dc:source und dc:title)
      - $OPENREFINE_CLIENT -P $PORT --apply ../../../rules/wuppertal/html.json $PROJECT
      # DDC einheitlich auf drei Ziffern vereinheitlichen (betrifft dc:subjects und oai:setSpec)
      - $OPENREFINE_CLIENT -P $PORT --apply ../../../rules/wuppertal/ddc.json $PROJECT
      # dc:publisher setzen
      - $OPENREFINE_CLIENT -P $PORT --apply ../../../rules/wuppertal/publisher.json $PROJECT
      # URNs, DOIs und PDF-Links aus dc:identifier extrahieren
      - $OPENREFINE_CLIENT -P $PORT --apply ../../../rules/wuppertal/identifier.json $PROJECT
      # Direktlinks generieren durch Abgleich der URNs mit nbn-resolving und Datensätze ohne Direktlink auf ein PDF löschen
      - $OPENREFINE_CLIENT -P $PORT --apply ../../../rules/wuppertal/nbn.json $PROJECT
      # Aufteilung dc:subject in ioo und topic
      - $OPENREFINE_CLIENT -P $PORT --apply ../../../rules/wuppertal/subjects.json $PROJECT
      # Standardisierte Rechteangaben Teil 1 (Links zu CC-Lizenzen)
      - $OPENREFINE_CLIENT -P $PORT --apply ../../../rules/wuppertal/rights.json $PROJECT
      # Datenstruktur für Templating vorbereiten: Pro Zeile ein Datensatz und leere Zeilen löschen
      - $OPENREFINE_CLIENT -P $PORT --apply ../../../rules/wuppertal/join.json $PROJECT
      # Zusammenführung gleichsprachiger Titelangaben zu Title/Subtitle
      - $OPENREFINE_CLIENT -P $PORT --apply ../../../rules/wuppertal/subtitle.json $PROJECT
      # Sprachangaben nach ISO-639-2b (betrifft dc:language sowie die xml:lang Attribute für dc:coverage, dc:description und dc:title)
      - $OPENREFINE_CLIENT -P $PORT --apply ../../../rules/wuppertal/language.json $PROJECT
      # Standardisierte Rechteangaben Teil 2 (Canonical Name für CC-Lizenzen)
      - $OPENREFINE_CLIENT -P $PORT --apply ../../../rules/wuppertal/rights-cc.json $PROJECT
      # Anreicherung HT-Nummer via lobid-resources
      - $OPENREFINE_CLIENT -P $PORT --apply ../../../rules/wuppertal/hbz.json $PROJECT
      # Sortierung mods:nonSort für das erste Element in dc:title
      - $OPENREFINE_CLIENT -P $PORT --apply ../../../rules/wuppertal/nonsort.json $PROJECT
      # Export in METS:MODS mit Templating
      - |
        $OPENREFINE_CLIENT -P $PORT --export --template "$(< ../../../rules/wuppertal/template.txt)" --rowSeparator "
        <!-- SPLIT -->
        " --output wuppertal.txt $PROJECT
      # Export für Debugging
      - $OPENREFINE_CLIENT -P $PORT --export --output wuppertal-debug.tsv $PROJECT
      # OpenRefine beenden
      - ps -o start,etime,%mem,%cpu,rss -p $(lsof -t -i:$PORT) # Statistik
      - kill -9 $(lsof -t -i:$PORT) # SIGKILL (-9) verhindert unnötigen Speichervorgang
      - rm -rf ./*.project* && rm -f workspace.json # temporäre Dateien von OpenRefine löschen
    sources:
      - ../harvest/wuppertal.xml
      - ../../../rules/wuppertal/*.json
      - ../../../rules/wuppertal/template.txt
#     - ../../../rules/common/*.json
    generates:
      - wuppertal.txt
      - wuppertal-debug.tsv

  check:
    dir: data/wuppertal/refine
    cmds:
      # Logdatei von OpenRefine auf Warnungen und Fehlermeldungen prüfen
      - if grep -i 'exception\|error' openrefine.log; then echo 1>&2 "Logdatei $PWD/openrefine.log enthält Warnungen!" && exit 1; fi
      # Prüfen, ob Mindestanzahl von 1300 Datensätzen generiert wurde
      - if (( 1300 > $(grep -c recordIdentifier wuppertal.txt) )); then echo 1>&2 "Unerwartet geringe Anzahl an Datensätzen in $PWD/wuppertal.txt!" && exit 1; fi

  split:
    dir: data/wuppertal/split
    cmds:
      # in Einzeldateien aufteilen
      - csplit -q ../refine/wuppertal.txt --suppress-matched '/<!-- SPLIT -->/' "{*}"
      # ggf. vorhandene XML-Dateien löschen
      - rm -f *.xml
      # Identifier als Dateinamen
      - for f in xx*; do mv "$f" "$(xmllint --xpath "//*[local-name(.) = 'recordIdentifier']/text()" "$f").xml"; done
    sources:
      - ../refine/wuppertal.txt
    generates:
      - ./*.xml

  validate:
    dir: data/wuppertal
    cmds:
      # Validierung gegen METS Schema
      - wget -q -nc https://www.loc.gov/standards/mets/mets.xsd
      - xmllint --schema mets.xsd --noout split/*.xml > validate.log 2>&1
    sources:
      - split/*.xml
    generates:
      - validate.log

  zip:
    dir: data/wuppertal
    cmds:
      # ZIP-Archiv mit Zeitstempel erstellen
      - zip -q -FS -j wuppertal_{{.DATE}}.zip split/*.xml
    sources:
      - split/*.xml
    generates:
      - wuppertal_{{.DATE}}.zip

  diff:
    dir: data/wuppertal
    cmds:
      # Inhalt der beiden letzten ZIP-Archive vergleichen
      - unzip -q -d old $(ls -t *.zip | sed -n 2p)
      - unzip -q -d new $(ls -t *.zip | sed -n 1p)
      - diff -d old new > diff.log || exit 0
      - rm -rf old new
      # Diff prüfen, ob es weniger als 500 Zeilen enthält
      - if (( 500 < $(wc -l <diff.log) )); then echo 1>&2 "Unerwartet große Änderungen in $PWD/diff.log!" && exit 1; fi
    sources:
      - split/*.xml
    generates:
      - diff.log
    status:
      # Task nicht ausführen, wenn weniger als zwei ZIP-Archive vorhanden
      - test -z $(ls -t *.zip | sed -n 2p)

  linkcheck:
    dir: data/wuppertal
    desc: links überprüfen
    cmds:
      # Links extrahieren
      - xmllint --xpath '//@*[local-name(.) = "href"]' split/*.xml | cut -d '"' -f2 > links.txt
      # http status code aller Links ermitteln
      - curl --silent --head --write-out "%{http_code} %{url_effective}\n" $(while read line; do echo "-o /dev/null $line"; done < links.txt) > linkcheck.log
      - rm -rf links.txt
      # Logdatei auf status code != 2XX prüfen
      - if grep '^[^2]' linkcheck.log; then echo 1>&2 "Logdatei $PWD/linkcheck.log enthält problematische status codes!" && exit 1; fi
    sources:
      - split/*.xml
    generates:
      - linkcheck.log

  delete:
    desc: cache löschen
    cmds:
      - rm -rf data/wuppertal/harvest
      - rm -rf data/wuppertal/refine
      - rm -rf data/wuppertal/split
